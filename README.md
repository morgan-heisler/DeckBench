# DECKBench: A Benchmark for Multi-Agent Slide Generation and Editing
![HuggingFace](https://img.shields.io/badge/huggingface-%23FFD21E.svg?style=for-the-badge&logo=huggingface&logoColor=white)
[![Hugging Face](https://img.shields.io/npm/v/npm.svg?logo=nodedotjs)](https://huggingface.co/datasets/mheisler/DeckBench)

This repository contains the official benchmark and evaluation code for **DECKBench**, a reproducible benchmark for **academic paperâ€“toâ€“slide generation and multi-turn slide editing**.

DECKBench evaluates the *full presentation workflow*, from converting long research papers into slide decks to iteratively refining those decks through natural-language editing instructions. The benchmark is designed for evaluating **LLM- and agent-based systems** under realistic, multi-turn conditions.

ðŸ“„ **Paper**: *DECKBench: Benchmarking Multi-Agent Slide Generation and Editing from Academic Papers*  
ðŸ§ª **Status**: KDD 2025 submission  
ðŸ“¦ **Release**: Post-submission / arXiv

---

## Overview

Preparing academic slide decks is an iterative, design-intensive process. Existing benchmarks typically evaluate either:
- one-shot text-to-slide generation, or
- isolated interface manipulation tasks.

DECKBench unifies these perspectives by introducing two tightly coupled tasks:

1. **Slide Generation**  
   Generate a complete academic slide deck from a full research paper.

2. **Multi-Turn Slide Editing**  
   Iteratively refine an existing slide deck in response to natural-language editing instructions.

The benchmark includes:
- curated paperâ€“slide pairs
- simulated multi-turn editing trajectories
- reference-free and reference-based evaluation metrics
- layout and visual design heuristics

---

## Repository Structure
```
deckbench/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ metadata/ # Paper and slide metadata
â”‚ â”œâ”€â”€ splits/ # Train / validation / test splits
â”‚ â””â”€â”€ edit_traces/ # Simulated multi-turn editing instructions
â”‚
â”œâ”€â”€ tasks/
â”‚ â”œâ”€â”€ generation/ # Paper â†’ slides task
â”‚ â””â”€â”€ editing/ # Multi-turn editing task
â”‚
â”œâ”€â”€ baselines/
â”‚ â””â”€â”€ multi_agent/ # Reference multi-agent baseline
â”‚
â”œâ”€â”€ evaluation/
â”‚ â”œâ”€â”€ slide_level.py # Slide-level metrics
â”‚ â”œâ”€â”€ deck_level.py # Deck-level metrics
â”‚ â”œâ”€â”€ interaction_level.py # Multi-turn metrics
â”‚ â””â”€â”€ layout_metrics.py # Layout & design heuristics
â”‚
â”œâ”€â”€ tools/
â”‚ â”œâ”€â”€ fetch_data.py # Script to download papers and slides
â”‚ â”œâ”€â”€ pdf_parser/ # PDF parsing utilities
â”‚ â””â”€â”€ slide_renderer/ # HTML / PPTX rendering tools
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ run_generation.py
â”‚ â”œâ”€â”€ run_editing.py
â”‚ â””â”€â”€ evaluate.py
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## Data Access and Licensing

### Important Note

**This repository does not redistribute conference papers or presentation slides.**

Due to licensing restrictions, we instead provide:
- metadata for each benchmark instance
- scripts to automatically retrieve publicly available PDFs and slides from official sources

Users are responsible for complying with the original licenses of the retrieved materials.

### Fetching the Data

```bash
python tools/fetch_data.py \
  --output_dir data/raw/
```

The script:

- downloads paper PDFs from official conference or author pages
- retrieves publicly hosted presentation slides when available
- validates checksums against benchmark metadata

---

## Task 1: Slide Generation
### Task Definition

#### Input
- Full academic paper (PDF or structured text)

#### Output
- A complete slide deck (HTML or PPTX)

The generated deck does not need to match the reference slides exactly in length or ordering.

### Evaluation Metrics

#### Slide-Level
- Textual similarity (embedding-based)
- Faithfulness to source paper
- Visual similarity (VLM-based)

#### Deck-Level
- Content coverage and grounding
- Narrative coherence and ordering (DTW-based)
- LLM-as-judge coherence and completeness
- Layout and visual quality metrics

#### Running Slide Evaluation

```
python scripts/run_evaluation.py \
  --model your_model_name \
  --split test \
  --output outputs/generation/
```
  ---
  

## Task 2: Multi-Turn Slide Editing
### Task Definition

#### Input (per turn)
- Current slide deck
- Natural-language editing instruction*

#### Output
- Updated slide deck

Editing is evaluated over multiple turns, reflecting realistic revision workflows.

#### Simulated Editing Traces

*Editing instructions are generated by a simulated user agent that compares intermediate decks against ground-truth final slides. 

Instructions vary in:
- verbosity
- specificity
- focus (content, layout, structure)

### Evaluation Metrics

#### Reference-Free
- Instruction-following accuracy (LLM-as-judge)
- Edit consistency and locality

#### Reference-Based
- Per-turn improvement toward reference deck
- DTW distance reduction
- Narrative alignment improvement across turns

#### Running Slide Editing & Evaluation
```
python scripts/run_editing.py \
  --model your_model_name \
  --split test \
  --output outputs/editing/
```
---

## Reproducibility

All experiments in the paper can be reproduced using:
- the provided benchmark splits
- deterministic evaluation scripts
- fixed LLM prompts and agent configurations

Random seeds, prompt templates, and configuration files are logged automatically.

---

## Citation
```
@inproceedings{deckbench2026,
  title     = {DECKBench: Benchmarking Multi-Agent Slide Generation and Editing from Academic Papers},
  author    = {authors},
  booktitle = {International Conference on Machine Learning (ICML)},
  year      = {2026}
}
```
---

## License

Code is released under the MIT License.

Dataset metadata and scripts are provided for research purposes only. Users must comply with the licenses of the original papers and slides.

---

## Contact

For questions or issues, please open a GitHub issue.




